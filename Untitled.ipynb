{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de099ed0-11db-44ea-9960-fd70f6bb4499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DAG to produce test 'california_housing' data to pg  \"\"\"\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.hooks.base import BaseHook\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import logging\n",
    "\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "def get_engine(conn_id) -> Engine:\n",
    "    dbhook = BaseHook.get_hook(conn_id=conn_id)\n",
    "    engine = create_engine(dbhook.get_uri())\n",
    "    return engine\n",
    "\n",
    "\n",
    "def produce_to_pg() -> None:\n",
    "    data = fetch_california_housing()\n",
    "\n",
    "    dataset = np.concatenate(\n",
    "        [data[\"data\"], data[\"target\"].reshape([data[\"target\"].shape[0], 1])], axis=1\n",
    "    )\n",
    "\n",
    "    dataset = pd.DataFrame(\n",
    "        dataset, columns=data[\"feature_names\"] + data[\"target_names\"]\n",
    "    )\n",
    "\n",
    "    engine = get_engine(\"pg_connection\")\n",
    "\n",
    "    try:\n",
    "        dataset.to_sql(\"california_housing1\", engine)\n",
    "    except ValueError:\n",
    "        print('asdasd')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de371351-2d36-466b-9b28-71d81e2ff2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-04-11T13:26:54.446+0000\u001b[0m] {\u001b[34mbase.py:\u001b[0m83} INFO\u001b[0m - Using connection ID 'pg_connection' for task execution.\u001b[0m\n",
      "[\u001b[34m2024-04-11T13:26:54.449+0000\u001b[0m] {\u001b[34mbase.py:\u001b[0m83} INFO\u001b[0m - Using connection ID 'pg_connection' for task execution.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "produce_to_pg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af6c33a7-ed92-440a-80bd-65ae5d2791c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DAG to functional test amazon S3 \"\"\"\n",
    "from datetime import timedelta\n",
    "from random import randint\n",
    "\n",
    "import backoff\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "\n",
    "import logging\n",
    "\n",
    "from airflow.utils.dates import days_ago\n",
    "from botocore.exceptions import (\n",
    "    ConnectTimeoutError,\n",
    "    EndpointConnectionError,\n",
    "    ConnectionError,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "cid = \"s3_connection\"\n",
    "\n",
    "\n",
    "rand = randintrand = randint(1, 1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_text_file() -> None:\n",
    "    with open(f\"/tmp/test.txt\", \"w\") as fp:\n",
    "        test = f\"test\"\n",
    "        fp.write(test)\n",
    "\n",
    "        s3 = S3Hook(\n",
    "            cid,\n",
    "            transfer_config_args={\n",
    "                \"use_threads\": False,\n",
    "            },\n",
    "        )\n",
    "        s3.load_file(\n",
    "            \"/tmp/test.txt\", key=f\"my-test-file{rand}.txt\", bucket_name=\"airflow\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def delete_text_file() -> None:\n",
    "    s3 = S3Hook(\n",
    "        cid,\n",
    "        transfer_config_args={\n",
    "            \"use_threads\": False,\n",
    "        },\n",
    "    )\n",
    "    # Delete generated file from Minio\n",
    "    s3.delete_objects(bucket=\"airflow\", keys=f\"my-test-file{rand}.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "781033dc-b7af-45d7-bc3f-ee2f10c678ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-04-11T13:30:57.340+0000\u001b[0m] {\u001b[34mbase.py:\u001b[0m83} INFO\u001b[0m - Using connection ID 's3_connection' for task execution.\u001b[0m\n",
      "[\u001b[34m2024-04-11T13:30:57.341+0000\u001b[0m] {\u001b[34mconnection_wrapper.py:\u001b[0m378} INFO\u001b[0m - AWS Connection (conn_id='s3_connection', conn_type='aws') credentials retrieved from login and password.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "write_text_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214a465-7b01-4eba-96db-c6f9a8bf903e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
