"""DAG to functional test amazon S3 """
from datetime import timedelta
from random import randint

import backoff
from airflow import DAG
from airflow.operators.python import PythonOperator

from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import logging
import torch
import pandas as pd
import numpy as np
import datetime
from datetime import timedelta
from transformers import AutoTokenizer, AutoModel
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from itertools import product
from sklearn.metrics import make_scorer
from sklearn.model_selection import cross_val_score
import re
from sklearn.model_selection import train_test_split
import torch
from sklearn.decomposition import PCA
from airflow.utils.dates import days_ago
from botocore.exceptions import (
    ConnectTimeoutError,
    EndpointConnectionError,
    ConnectionError,
)
import os
import joblib
from catboost import CatBoostRegressor
import numpy as np
logger = logging.getLogger(__name__)
logger.addHandler(logging.StreamHandler())

cid = "s3_connection"

DEFAULT_ARGS = {
    "owner": "Alan",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=1),
}

dag = DAG(
    "moex_create_preprocess",
    tags=["pipe"],
    catchup=False,
    start_date=days_ago(2),
    default_args=DEFAULT_ARGS,
    schedule_interval="@once",
)

cid = "s3_connection"


def process_moex_data():
    dirs = set(os.listdir("/tmp/")) - set(
        ["models--cointegrated--rubert-tiny2", "requirements.txt", "tmp5sgasuflcacert.pem"])

    def remove_directory(path):
        if os.path.exists(path):
            for root, dirs, files in os.walk(path, topdown=False):
                for name in files:
                    file_path = os.path.join(root, name)
                    os.remove(file_path)
                for name in dirs:
                    dir_path = os.path.join(root, name)
                    os.rmdir(dir_path)
            os.rmdir(path)

    for d in dirs:
        if os.path.isdir(f"/tmp/{d}"):
            remove_directory(f"/tmp/{d}")
        else:
            os.remove(f"/tmp/{d}")
    s3 = S3Hook(
        cid,
        transfer_config_args={
            "use_threads": False,
        },
    )
    # os.remove("/tmp/moex.csv")
    s3.download_file(
        local_path="/tmp/", preserve_file_name=True, use_autogenerated_subdir=False, key="/moex_data/moex.csv",
        bucket_name="airflow"
    )

    with open('/tmp/moex.csv', 'r', encoding='cp1251') as fin:
        text = fin.read()
        text = text.lstrip('history').lstrip('\n')

    with open('/tmp/moex2.csv', 'w', encoding='utf-8') as out:
        out.write(text)

    moex = pd.read_csv('/tmp/moex2.csv', encoding='utf-8', sep=';')

    def cols_to_right_format(df):
        df.TRADEDATE = pd.to_datetime(df.TRADEDATE, format='%d.%m.%Y')
        df.CAPITALIZATION = df.CAPITALIZATION.str.replace(',', '.').astype(float) / 1e12
        df.DIVISOR = df.DIVISOR.str.replace(',', '.').astype(float) / 1e9
        for cat in ['OPEN', 'CLOSE', 'HIGH', 'LOW']:
            df[cat] = df[cat].str.replace(',', '.').astype(float)
        return df

    def drop_nans(df):
        drop_columns = ['NAME', 'SHORTNAME', 'SECID', 'BOARDID', 'DURATION', 'YIELD', 'DECIMALS', 'CURRENCYID',
                        'VOLUME', 'TRADINGSESSION', 'VALUE']
        df.drop(drop_columns, axis=1, inplace=True)
        df = df[df.CAPITALIZATION.notna()]
        df = df[df.DIVISOR.notna()]
        return df

    def add_new_features(df):
        df['RANGE_PAST'] = (df.HIGH - df.LOW).shift()
        df['CLOSE_PAST'] = df.CLOSE.shift()
        df['DAY'] = df.TRADEDATE.dt.day
        df['DAY_OF_WEEK'] = df.TRADEDATE.dt.dayofweek
        df['MONTH'] = df.TRADEDATE.dt.month
        df['CAPITALIZATION_PAST'] = df.CAPITALIZATION.shift()
        df['DIVISOR_PAST'] = df.DIVISOR.shift()
        return df

    def fix_indices(df):
        df = df[1:]
        df.reset_index(drop=True, inplace=True)
        return df

    pipe = Pipeline(
        [
            ('cols_to_right_format', FunctionTransformer(cols_to_right_format)),
            ('drop_nans', FunctionTransformer(drop_nans)),
            ('add_new_features', FunctionTransformer(add_new_features)),
            ('fix_indices', FunctionTransformer(fix_indices))
        ]
    )


    moex = pipe.transform(moex)

    moex.to_csv('/tmp/moex_clean.csv', index=False)
    os.remove("/tmp/moex.csv")
    os.remove("/tmp/moex2.csv")
    s3.load_file(
        filename="/tmp/moex_clean.csv", key=f"/moex_data/moex_clean.csv", bucket_name="airflow"
    )
    os.remove("/tmp/moex_clean.csv")
    joblib.dump(pipe, "moex_pipeline.plk")
    s3.load_file(
        "moex_pipeline.plk", key=f"/moex_data/moex_pipeline.plk", bucket_name="airflow"
    )

gen_emmbeding = PythonOperator(
    task_id="moex_proccess_create",
    provide_context=True,
    python_callable=process_moex_data,
    dag=dag,
)

